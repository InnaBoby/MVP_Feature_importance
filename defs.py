# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWA2qantte9RRsmB8gT6F_C5Tzo8w4Pr
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

from catboost import CatBoostRegressor

import matplotlib.pyplot as plt
import seaborn as sns

pd.options.display.float_format ='{:,.3f}'.format
pd.set_option('display.max_columns', None)

#N_PAST = 29 # Сколько недель прошлого берем
#N_FUTURE = 29 # Горизонт предикта

def data_preprocess(data):
    data = data.drop(columns=['Начало нед']).applymap(lambda x: float(str(x).replace(',', '') if x!=0 else 0.001))

    #делаем сдвиг по таргету на 1 период
    data['KPI \nданные понедельно Продажи, рубли'] = data['KPI \nданные понедельно Продажи, рубли']. shift (1)
    data = data[1:-1]
    return data

def create_sequences(input_data, target_data, N_PAST, N_FUTURE):
    X, y = [], []
    for i in range(N_PAST, len(input_data) - N_FUTURE +1):
        X.append(input_data[i - N_PAST:i])
        y.append(target_data[i:i + N_FUTURE])
    return np.array(X), np.array(y)

#LSTM
class LSTM_for_features(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers=2, n_hidden=256):
        super(LSTM_for_features, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.n_hidden=n_hidden
        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)
        self.in2output = nn.Linear(input_size + hidden_size, output_size)
        self.lstm = nn.LSTM(input_size, hidden_size, n_layers,
                            batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, batch_size):
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, x.size(0), self.n_hidden).zero_(),
                      weight.new(self.n_layers, x.size(0), self.n_hidden).zero_())
        output, (hid,_) = self.lstm(x, hidden)
        hid.view(-1, self.hidden_size)
        out = self.fc(output)
        return out, hid



#Training Loop
def training_model(model,
                   train_loader,
                   val_loader,
                   num_epochs,
                   optimizer,
                   criterion,
                   scheduler,
                   batch_size):

    best_val_loss = float('inf')
    train_losses, val_losses = [], []
    scheduler_patience = 10
    scheduler_break = 10

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs, hid = model(inputs.unsqueeze(1), batch_size)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        train_losses.append(total_loss / len(train_loader))

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs, hid = model(inputs.unsqueeze(1), batch_size)
                val_loss = criterion(outputs, targets)
                total_val_loss += val_loss.item()
        val_loss = total_val_loss / len(val_loader)
        val_losses.append(val_loss)

        if epoch % 50 == 0 or epoch == num_epochs - 1:
            print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')
            counter = 0
        else:
            counter += 1

        if counter >= scheduler_patience:
            model.load_state_dict(torch.load('best_model.pth'))
            # model = model.to(device)
            scheduler.step()
            print('new LR:', optimizer.param_groups[0]["lr"])
            counter = 0
            scheduler_break -= 1

        if scheduler_break < 0:
            break

    print("Training complete.")
    return model, train_losses, val_losses


def dataloaders(data, target, feature_cols, N_PAST, N_FUTURE, batch_size):
    for col in feature_cols:


      X = np.array(data[col][:-N_FUTURE])
      y=np.array(data[col][N_FUTURE:])
      #y=np.array(df[target])

      X_train, X_test, X_val = X[ :len(data)-N_FUTURE*5], X[len(data)-N_FUTURE*5:len(data)-N_FUTURE*3], X[len(data)-N_FUTURE*3:]
      y_train, y_test, y_val = y[ :len(data)-N_FUTURE*5], y[len(data)-N_FUTURE*5:len(data)-N_FUTURE*3], y[len(data)-N_FUTURE*3:]

      scaler = StandardScaler()
      X_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(1, -1)[0]
      X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(1, -1)[0]
      X_val=scaler.transform(X_val.reshape(-1, 1)).reshape(1, -1)[0]

      y_train = scaler.fit_transform(y_train.reshape(-1, 1)).reshape(1, -1)[0]
      y_test = scaler.transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]
      y_val=scaler.transform(y_val.reshape(-1, 1)).reshape(1, -1)[0]

      X_train_seq, y_train_seq = create_sequences(X_train, y_train, N_PAST, N_FUTURE)
      X_test_seq, y_test_seq = create_sequences(X_test, y_test, N_PAST, N_FUTURE)
      X_val_seq, y_val_seq = create_sequences(X_val, y_val, N_PAST, N_FUTURE)

      X_train_seq_tensor = torch.tensor(X_train_seq, dtype=torch.float32)
      y_train_seq_tensor = torch.tensor(y_train_seq, dtype=torch.float32)
      X_test_seq_tensor = torch.tensor(X_test_seq, dtype=torch.float32)
      y_test_seq_tensor = torch.tensor(y_test_seq, dtype=torch.float32)
      X_val_seq_tensor = torch.tensor(X_val_seq, dtype=torch.float32)
      y_val_seq_tensor = torch.tensor(y_val_seq, dtype=torch.float32)

      train_dataset = TensorDataset(X_train_seq_tensor, y_train_seq_tensor)
      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
      test_dataset = TensorDataset(X_test_seq_tensor, y_test_seq_tensor)
      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
      val_dataset = TensorDataset(X_val_seq_tensor, y_val_seq_tensor)
      val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
      return train_loader, test_loader, val_loader





def LSTM_go(feature_cols,
            batch_size,
            model,
            train_loader,
            val_loader,
            num_epochs,
            optimizer,
            criterion,
            scheduler, pred_data):
    for col in feature_cols:
      model, train_losses, val_losses = training_model(model=model,
                                                     train_loader=train_loader,
                                                     val_loader=val_loader,
                                                     num_epochs=num_epochs,
                                                     optimizer=optimizer,
                                                     criterion=criterion,
                                                     scheduler=scheduler,
                                                     batch_size=batch_size)
      model.eval()
      predictions, actuals = [], []
      with torch.no_grad():
        for inputs, targets in val_loader:
            outputs, hid = model(inputs.unsqueeze(1), batch_size)
            predictions.extend(outputs.numpy())
            actuals.extend(targets.view(targets.size(0), -1).numpy())

      rmse = np.sqrt(np.mean((pd.Series(predictions[0][0]) - pd.Series(actuals[0]))**2))
      mape = np.mean(np.abs((pd.Series(actuals[0]) - pd.Series(predictions[0][0])) / pd.Series(actuals[0]))) * 100

      print('RMSE и MAPE:', rmse, mape)
     # preds = scaler.inverse_transform(predictions[0])
     # pred_data[f'Pred_{col}'] = preds[0]
      pred_data[f'Pred_{col}'] = pd.Series(predictions[0][0])
     # pred_data[f'Actual_{col}'] = pd.Series(actuals[0])

      return pred_data




def Catboost_go(data, feature_cols, target):
    X = data[feature_cols]
    y = data[target]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)

    scaler = StandardScaler()
    X_train=scaler.fit_transform(X_train)
    X_test=scaler.transform(X_test)
    y_train=scaler.fit_transform(np.array(y_train).reshape(-1, 1)).reshape(1, -1)[0]
    y_test=scaler.transform(np.array(y_test).reshape(-1, 1)).reshape(1, -1)[0]

    model = CatBoostRegressor(iterations=50,
                              learning_rate=0.1,
                              depth=5)

    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    #rmse = np.sqrt(np.mean((preds - y_test)**2))
    return preds, model




def plot_feature_importance(importance,names,model_type):

    #Create arrays from feature importance and feature names
    feature_importance = np.array(importance)
    feature_names = np.array(names)

    #Create a DataFrame using a Dictionary
    data={'feature_names':feature_names,'feature_importance':feature_importance}
    fi_df = pd.DataFrame(data)

    #Sort the DataFrame in order decreasing feature importance
    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

    #Define size of bar plot
    fig, ax = plt.subplot(figsize=(10,8))
    #Plot Searborn bar chart
    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
    #Add chart labels
    ax.title(model_type + 'FEATURE IMPORTANCE')
    ax.xlabel('FEATURE IMPORTANCE')
    ax.ylabel('FEATURE NAMES')
    return fig

def feature_importance_table(importance, names, feature_cols):
    feat_imp = pd.DataFrame({'feature_importance': importance,
                             'feature_names': names})
    feat_imp['Names'] = feature_cols
    return feat_imp.sort_values(by=['feature_importance'], ascending=False).head(50)


def predict_cash(pred_data, model, subm):
    res = model.predict(pred_data)

    res = scaler.inverse_transform(np.array(res).reshape(-1, 1)).reshape(1, -1)[0]
    subm['revenue'] = res
    return subm


